{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multiple_pdf(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Read multiple PDF files from the specified file path and extract the text from each page.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The directory path containing the PDF files.\n",
    "    Returns:\n",
    "        list: A list containing the extracted text from each page of the PDF files.\n",
    "    \"\"\"\n",
    "    pdf_files = get_pdf_files(file_path)\n",
    "    output = []\n",
    "    for file in pdf_files:\n",
    "        try:\n",
    "            with open(file, \"rb\") as f:\n",
    "                pdf_reader = PdfReader(f)\n",
    "                count =get_filenames_from_dirput.append(page.extractText())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file '{file}': {str(e)}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read a single PDF file and extract the text from each page.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path of the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the extracted text from each page of the PDF file.\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            pdf_reader = PdfReader(f)\n",
    "            count = len(pdf_reader.pages)\n",
    "            for i in range(count):\n",
    "                page = pdf_reader.pages[i]\n",
    "                output.append(page.extractText())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{file_path}': {str(e)}\")\n",
    "    return str(\" \".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    def __init__(self, raw_text):\n",
    "        self.stopwords_set = set(stopwords.words(\n",
    "            \"english\") + list(string.punctuation))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.raw_input_text = raw_text\n",
    "\n",
    "    def clean_text(self) -> str:\n",
    "        tokens = word_tokenize(self.raw_input_text.lower())\n",
    "        tokens = [token for token in tokens if token not in self.stopwords_set]\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load the English Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "REGEX_PATTERNS = {\n",
    "    \"email_pattern\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\",\n",
    "    \"phone_pattern\": r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n",
    "    \"link_pattern\": r\"\\b(?:https?://|www\\.)\\S+\\b\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_id():\n",
    "    \"\"\"\n",
    "    Generate a unique ID and return it as a string\n",
    "    Returns:\n",
    "        str: A string with a unique ID.\n",
    "    \"\"\"\n",
    "    return str(uuid4())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    \"\"\"\n",
    "    A class for cleaning a text by removing specific patterns\n",
    "    \"\"\"\n",
    "\n",
    "    def remove_emails_links(text):\n",
    "        \"\"\"\n",
    "        Clean the input text by removing specific patterns\n",
    "        Args:\n",
    "            text (str): The input text to clean\n",
    "        Returns:\n",
    "            str: The cleaned text\n",
    "        \"\"\"\n",
    "        for pattern in REGEX_PATTERNS:\n",
    "            text = re.sub(REGEX_PATTERNS[pattern], \"\", text)\n",
    "        return text\n",
    "\n",
    "    def clean_text(text):\n",
    "        \"\"\"\n",
    "        Clean the input text by removing specific patterns\n",
    "\n",
    "        Args:\n",
    "            text(str) : the input to clean\n",
    "        Returns:\n",
    "            str: the cleaned text\n",
    "        \"\"\"\n",
    "        text = TextCleaner.remove_emails_links(text)\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PUNCT\":\n",
    "                text = text.replace(token.text, \"\")\n",
    "        return str(text)\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        \"\"\"\n",
    "        Clean the input text by removing stopwords.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to clean.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text.\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if token.is_stop:\n",
    "                text = text.replace(token.text, \"\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountFrequency:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.doc = nlp(text)\n",
    "\n",
    "    def count_frequency(self):\n",
    "        \"\"\"\n",
    "        Count the frequency of words in the input text\n",
    "        Returns:\n",
    "            dict: a dictionary with the words as keys and the frequency as values\n",
    "        \"\"\"\n",
    "\n",
    "        pos_freq = {}\n",
    "        for token in self.doc:\n",
    "            if token.pos_ in pos_freq:\n",
    "                pos_freq[token.pos_] += 1\n",
    "            else:\n",
    "                pos_freq[token.pos_] = 1\n",
    "        return pos_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeyTerms Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 09:52:55.137417: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-01 09:52:57.268312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-01 09:52:58.979287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 09:52:58.987388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 09:52:58.987608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "from textacy import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeytermExtractor:\n",
    "    \"\"\"\n",
    "    A class for extracting keyterms from a given text using various algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_text: str, top_n_values: int = 20):\n",
    "        \"\"\"\n",
    "        Initialize the KeytermExtractor object.\n",
    "\n",
    "        Args:\n",
    "            raw_text (str): The raw input text.\n",
    "            top_n_values (int): The number of top keyterms to extract.\n",
    "        \"\"\"\n",
    "        self.raw_text = raw_text\n",
    "        self.text_doc = textacy.make_spacy_doc(self.raw_text, lang=\"en_core_web_md\")\n",
    "        self.top_n_values = top_n_values\n",
    "\n",
    "    def get_keyterms_based_on_textrank(self):\n",
    "        \"\"\"\n",
    "        Extract keyterms using the TextRank algorithm.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of top keyterms based on TextRank.\n",
    "        \"\"\"\n",
    "        return list(\n",
    "            extract.keyterms.textrank(\n",
    "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_keyterms_based_on_sgrank(self):\n",
    "        \"\"\"\n",
    "        Extract keyterms using the SGRank algorithm.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of top keyterms based on SGRank.\n",
    "        \"\"\"\n",
    "        return list(\n",
    "            extract.keyterms.sgrank(\n",
    "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_keyterms_based_on_scake(self):\n",
    "        \"\"\"\n",
    "        Extract keyterms using the sCAKE algorithm.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of top keyterms based on sCAKE.\n",
    "        \"\"\"\n",
    "        return list(\n",
    "            extract.keyterms.scake(\n",
    "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_keyterms_based_on_yake(self):\n",
    "        \"\"\"\n",
    "        Extract keyterms using the YAKE algorithm.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of top keyterms based on YAKE.\n",
    "        \"\"\"\n",
    "        return list(\n",
    "            extract.keyterms.yake(\n",
    "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def bi_gramchunker(self):\n",
    "        \"\"\"\n",
    "        Chunk the text into bigrams.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of bigrams.\n",
    "        \"\"\"\n",
    "        return list(\n",
    "            textacy.extract.basics.ngrams(\n",
    "                self.text_doc,\n",
    "                n=2,\n",
    "                filter_stops=True,\n",
    "                filter_nums=True,\n",
    "                filter_punct=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def tri_gramchunker(self):\n",
    "        \"\"\"\n",
    "        Chunk the text into trigrams.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of trigrams.\n",
    "        \"\"\"\n",
    "        return list(\n",
    "            textacy.extract.basics.ngrams(\n",
    "                self.text_doc,\n",
    "                n=3,\n",
    "                filter_stops=True,\n",
    "                filter_nums=True,\n",
    "                filter_punct=True,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_SECTIONS = [\n",
    "    \"Contact Information\",\n",
    "    \"Objective\",\n",
    "    \"Summary\",\n",
    "    \"Education\",\n",
    "    \"Experience\",\n",
    "    \"Skills\",\n",
    "    \"Projects\",\n",
    "    \"Certifications\",\n",
    "    \"Licenses\",\n",
    "    \"Awards\",\n",
    "    \"Honors\",\n",
    "    \"Publications\",\n",
    "    \"References\",\n",
    "    \"Technical Skills\",\n",
    "    \"Computer Skills\",\n",
    "    \"Programming Languages\",\n",
    "    \"Software Skills\",\n",
    "    \"Soft Skills\",\n",
    "    \"Language Skills\",\n",
    "    \"Professional Skills\",\n",
    "    \"Transferable Skills\",\n",
    "    \"Work Experience\",\n",
    "    \"Professional Experience\",\n",
    "    \"Employment History\",\n",
    "    \"Internship Experience\",\n",
    "    \"Volunteer Experience\",\n",
    "    \"Leadership Experience\",\n",
    "    \"Research Experience\",\n",
    "    \"Teaching Experience\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    A class for extracting various types of data from text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_text: str):\n",
    "        \"\"\"\n",
    "        Initialize the DataExtractor object.\n",
    "\n",
    "        Args:\n",
    "            raw_text (str): The raw input text.\n",
    "        \"\"\"\n",
    "\n",
    "        self.text = raw_text\n",
    "        self.clean_text = TextCleaner.clean_text(self.text)\n",
    "        self.doc = nlp(self.clean_text)\n",
    "\n",
    "    def extract_links(self):\n",
    "        \"\"\"\n",
    "        Find links of any type in a given string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The string to search for links.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing all the found links.\n",
    "        \"\"\"\n",
    "        link_pattern = r\"\\b(?:https?://|www\\.)\\S+\\b\"\n",
    "        links = re.findall(link_pattern, self.text)\n",
    "        return links\n",
    "\n",
    "    def extract_links_extended(self):\n",
    "        \"\"\"\n",
    "        Extract links of all kinds (HTTP, HTTPS, FTP, email, www.linkedin.com,\n",
    "          and github.com/user_name) from a webpage.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing all the extracted links.\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        try:\n",
    "            response = urllib.request.urlopen(self.text)\n",
    "            html_content = response.read().decode(\"utf-8\")\n",
    "            pattern = r'href=[\\'\"]?([^\\'\" >]+)'\n",
    "            raw_links = re.findall(pattern, html_content)\n",
    "            for link in raw_links:\n",
    "                if link.startswith(\n",
    "                    (\n",
    "                        \"http://\",\n",
    "                        \"https://\",\n",
    "                        \"ftp://\",\n",
    "                        \"mailto:\",\n",
    "                        \"www.linkedin.com\",\n",
    "                        \"github.com/\",\n",
    "                        \"twitter.com\",\n",
    "                    )\n",
    "                ):\n",
    "                    links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting links: {str(e)}\")\n",
    "        return links\n",
    "\n",
    "    def extract_names(self):\n",
    "        \"\"\"Extracts and returns a list of names from the given\n",
    "        text using spaCy's named entity recognition.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to extract names from.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of strings representing the names extracted from the text.\n",
    "        \"\"\"\n",
    "        names = [ent.text for ent in self.doc.ents if ent.label_ == \"PERSON\"]\n",
    "        return names\n",
    "\n",
    "    def extract_emails(self):\n",
    "        \"\"\"\n",
    "        Extract email addresses from a given string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The string from which to extract email addresses.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing all the extracted email addresses.\n",
    "        \"\"\"\n",
    "        email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "        emails = re.findall(email_pattern, self.text)\n",
    "        return emails\n",
    "\n",
    "    def extract_phone_numbers(self):\n",
    "        \"\"\"\n",
    "        Extract phone numbers from a given string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The string from which to extract phone numbers.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing all the extracted phone numbers.\n",
    "        \"\"\"\n",
    "        phone_number_pattern = (\n",
    "            r\"^(\\+\\d{1,3})?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}$\"\n",
    "        )\n",
    "        phone_numbers = re.findall(phone_number_pattern, self.text)\n",
    "        return phone_numbers\n",
    "\n",
    "    def extract_experience(self):\n",
    "        \"\"\"\n",
    "        Extract experience from a given string. It does so by using the Spacy module.\n",
    "\n",
    "        Args:\n",
    "            text (str): The string from which to extract experience.\n",
    "\n",
    "        Returns:\n",
    "            str: A string containing all the extracted experience.\n",
    "        \"\"\"\n",
    "        experience_section = []\n",
    "        in_experience_section = False\n",
    "\n",
    "        for token in self.doc:\n",
    "            if token.text in RESUME_SECTIONS:\n",
    "                if token.text == \"Experience\" or \"EXPERIENCE\" or \"experience\":\n",
    "                    in_experience_section = True\n",
    "                else:\n",
    "                    in_experience_section = False\n",
    "\n",
    "            if in_experience_section:\n",
    "                experience_section.append(token.text)\n",
    "\n",
    "        return \" \".join(experience_section)\n",
    "\n",
    "    def extract_position_year(self):\n",
    "        \"\"\"\n",
    "        Extract position and year from a given string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The string from which to extract position and year.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing the extracted position and year.\n",
    "        \"\"\"\n",
    "        position_year_search_pattern = (\n",
    "            r\"(\\b\\w+\\b\\s+\\b\\w+\\b),\\s+(\\d{4})\\s*-\\s*(\\d{4}|\\bpresent\\b)\"\n",
    "        )\n",
    "        position_year = re.findall(position_year_search_pattern, self.text)\n",
    "        return position_year\n",
    "\n",
    "    def extract_particular_words(self):\n",
    "        \"\"\"\n",
    "        Extract nouns and proper nouns from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to extract nouns from.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of extracted nouns.\n",
    "        \"\"\"\n",
    "        pos_tags = [\"NOUN\", \"PROPN\"]\n",
    "        nouns = [token.text for token in self.doc if token.pos_ in pos_tags]\n",
    "        return nouns\n",
    "\n",
    "    def extract_entities(self):\n",
    "        \"\"\"\n",
    "        Extract named entities of types 'GPE' (geopolitical entity) and 'ORG' (organization) from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to extract entities from.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of extracted entities.\n",
    "        \"\"\"\n",
    "        entity_labels = [\"GPE\", \"ORG\"]\n",
    "        entities = [\n",
    "            token.text for token in self.doc.ents if token.label_ in entity_labels\n",
    "        ]\n",
    "        return list(set(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Job Desc to Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIRECTORY = \"Data/Processed/JobDescription\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseJobDesc:\n",
    "    def __init__(self, job_desc: str):\n",
    "        self.job_desc_data = job_desc\n",
    "        self.clean_data = TextCleaner.clean_text(self.job_desc_data)\n",
    "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
    "        self.key_words = DataExtractor(\n",
    "            self.clean_data).extract_particular_words()\n",
    "        self.pos_frequencies = CountFrequency(\n",
    "            self.clean_data).count_frequency()\n",
    "        self.keyterms = KeytermExtractor(\n",
    "            self.clean).get_keyterms_based_on_sgrank()\n",
    "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
    "        self.tri_grmas = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
    "\n",
    "    def get_JSON(self) -> dict:\n",
    "        \"\"\"\n",
    "        Returns a dictionary of job description data.\n",
    "        \"\"\"\n",
    "        job_desc_dictionary = {\n",
    "            \"unique_id\": generate_unique_id(),\n",
    "            \"job_desc_data\": self.job_desc_data,\n",
    "            \"clean_data\": self.clean_data,\n",
    "            \"entities\": self.entities,\n",
    "            \"extracted_keywords\": self.key_words,\n",
    "            \"keyterms\": self.keyterms,\n",
    "            \"bi_grams\": str(self.bi_grams),\n",
    "            \"tri_grams\": str(self.tri_grmas),\n",
    "            \"pos_frequencies\": self.pos_frequencies,\n",
    "        }\n",
    "\n",
    "        return job_desc_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Resume to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "SAVE_DIRECTORY = \"Data/Processed/Resumes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseResume:\n",
    "    def __init__(self, resume: str):\n",
    "        self.resume_data = resume\n",
    "        self.clean_data = TextCleaner.clean_text(self.resume_data)\n",
    "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
    "        self.name = DataExtractor(self.clean_data[:30]).extract_names()\n",
    "        self.experience = DataExtractor(self.clean_data).extract_experience()\n",
    "        self.emails = DataExtractor(self.resume_data).extract_emails()\n",
    "        self.phones = DataExtractor(self.resume_data).extract_phone_numbers()\n",
    "        self.years = DataExtractor(self.clean_data).extract_position_year()\n",
    "        self.key_words = DataExtractor(\n",
    "            self.clean_data).extract_particular_words()\n",
    "        self.pos_frequencies = CountFrequency(\n",
    "            self.clean_data).count_frequency()\n",
    "        self.keyterms = KeytermExtractor(\n",
    "            self.clean_data).get_keyterms_based_on_sgrank()\n",
    "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
    "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
    "\n",
    "    def get_JSON(self) -> dict:\n",
    "        \"\"\"\n",
    "        Returns a dictionary of resume data.\n",
    "        \"\"\"\n",
    "        resume_dictionary = {\n",
    "            \"unique_id\": generate_unique_id(),\n",
    "            \"resume_data\": self.resume_data,\n",
    "            \"clean_data\": self.clean_data,\n",
    "            \"entities\": self.entities,\n",
    "            \"extracted_keywords\": self.key_words,\n",
    "            \"keyterms\": self.keyterms,\n",
    "            \"name\": self.name,\n",
    "            \"experience\": self.experience,\n",
    "            \"emails\": self.emails,\n",
    "            \"phones\": self.phones,\n",
    "            \"years\": self.years,\n",
    "            \"bi_grams\": str(self.bi_grams),\n",
    "            \"tri_grams\": str(self.tri_grams),\n",
    "            \"pos_frequencies\": self.pos_frequencies,\n",
    "        }\n",
    "\n",
    "        return resume_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JobDescription processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_JOB_DESCRIPTION_FROM = \"Data/JobDescription\"\n",
    "SAVE_DIRECTORY = \"Data/Processed/JobDescription\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDescriptionProcessor:\n",
    "    def __init__(self, input_file):\n",
    "        self.input_file = input_file\n",
    "        self.input_file_name = os.path.join(\n",
    "            READ_JOB_DESCRIPTION_FROM + self.input_file)\n",
    "\n",
    "    def process(self) -> bool:\n",
    "        try:\n",
    "            resume_dict = self._read_resumes()\n",
    "            self._write_json_file(resume_dict)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _read_resumes(self) -> dict:\n",
    "        data = read_single_pdf(self.input_file_name)\n",
    "        output = ParseResume(data).get_JSON\n",
    "        return output\n",
    "\n",
    "    def _write_json_file(self, resume_dictionary: dict):\n",
    "        file_name = str(\n",
    "            \"JobDescription-\"\n",
    "            + self.input_file\n",
    "            + resume_dictionary[\"unique_id\"]\n",
    "            + \".json\"\n",
    "        )\n",
    "        save_dictionary_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
    "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
    "        with open(save_dictionary_name, \"w+\") as outfile:\n",
    "            outfile.write(json_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_RESUME_FROM = \"Data/Resumes/\"\n",
    "SAVE_DIRECTORY = \"Data/Processed/Resumes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeProcessor:\n",
    "    def __init__(self, input_file):\n",
    "        self.input_file = input_file\n",
    "        self.input_file_name = os.path.join(READ_RESUME_FROM + self.input_file)\n",
    "\n",
    "    def process(self) -> bool:\n",
    "        try:\n",
    "            resume_dict = self._read_resumes()\n",
    "            self._write_json_file(resume_dict)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _read_resumes(self) -> dict:\n",
    "        data = read_single_pdf(self.input_file_name)\n",
    "        output = ParseResume(data).get_JSON()\n",
    "        return output\n",
    "\n",
    "    def _read_job_desc(self) -> dict:\n",
    "        data = read_single_pdf(self.input_file_name)\n",
    "        output = ParseJobDesc(data).get_JSON()\n",
    "        return output\n",
    "\n",
    "    def _write_json_file(self, resume_dictionary: dict):\n",
    "        file_name = str(\n",
    "            \"Resume-\" + self.input_file + resume_dictionary[\"unique_id\"] + \".json\"\n",
    "        )\n",
    "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
    "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
    "        with open(save_directory_name, \"w+\") as outfile:\n",
    "            outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Batch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path(folder_name):\n",
    "    curr_dir = os.getcwd()\n",
    "    while True:\n",
    "        if folder_name in os.listdir(curr_dir):\n",
    "            return os.path.join(curr_dir, folder_name)\n",
    "        else:\n",
    "            parent_dir = os.path.dirname(curr_dir)\n",
    "            if parent_dir == \"/\":\n",
    "                break\n",
    "            curr_dir = parent_dir\n",
    "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
    "\n",
    "\n",
    "cwd = find_path(\"Resume_Matcher\")\n",
    "READ_RESUME_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"Resumes\")\n",
    "READ_JOB_DESCRIPTION_FROM = os.path.join(\n",
    "    cwd, \"Data\", \"Processed\", \"JobDescription\")\n",
    "config_path = os.path.join(cwd, \"scripts\", \"similiarty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path) as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            data = {}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantSearch:\n",
    "    def __init__(self, resumes, jd):\n",
    "        self.qdrant_key = \"\"\n",
    "        self.qdrant_url = \"\"\n",
    "        self.resumes = resumes\n",
    "        self.jd = jd\n",
    "        self.qdrant = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_key)\n",
    "        # Vector_size = 4096\n",
    "        vector_size = 384\n",
    "        self.qdrant.recreate_collection(\n",
    "            collection_name=\"collection_resume_matcher\",\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=vector_size, distance=models.Distance.COSINE\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        try:\n",
    "            embeddings = model.encode([text])\n",
    "            return list(map(float, embeddings[0])), len(embeddings[0])\n",
    "        except Exception as e:\n",
    "            print(\"ERROR while Embeddign\")\n",
    "\n",
    "    def update_qdrant(self):\n",
    "        vectors = []\n",
    "        ids = []\n",
    "        for i, resume in enumerate(self.resumes):\n",
    "            vector, size = self.get_embedding(resume)\n",
    "            vectors.append(vector)\n",
    "            ids.append(i)\n",
    "        try:\n",
    "            self.qdrant.upsert(\n",
    "                collection_name=\"collection_resume_matcher\",\n",
    "                points=Batch(\n",
    "                    ids=ids,\n",
    "                    vectors=vectors,\n",
    "                    payloads=[{\"text\": resume} for resume in self.resumes],\n",
    "                ),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error updating the vectors to the qdrant collection: {e}\",\n",
    "                exc_info=True,\n",
    "            )\n",
    "\n",
    "    def search(self):\n",
    "        vector, _ = self.get_embedding(self.jd)\n",
    "        hits = self.qdrant.search(\n",
    "            collection_name=\"collection_resume_matcher\", query_vector=vector, limit=30\n",
    "        )\n",
    "        results = []\n",
    "        for hit in hits:\n",
    "            result = {\"text\": str(hit.payload)[:30], \"score\": hit.score}\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similiarty_score(resume_string, job_description_string):\n",
    "    qdrant_search = QdrantSearch([resume_string], job_description_string)\n",
    "    qdrant_search.update_qdrant()\n",
    "    search_result = qdrant_search.search()\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dict = read_doc(\n",
    "    READ_RESUME_FROM + \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_RESUMES_PATH = \"Data/Processed/Resumes\"\n",
    "PROCESSED_JOB_DESCRIPTIONS_PATH = \"Data/Processed/JobDescription\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_old_files(files_path):\n",
    "    for filename in os.listdir(files_path):\n",
    "        try:\n",
    "            file_path = os.path.join(files_path, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.rename(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing file '{file_path}': {str(e)}\")\n",
    "    print(\"Finished removing old files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Processed/Resumes\n",
      "Error reading file '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesalfred_pennyworth_pm.pdf': [Errno 2] No such file or directory: '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesalfred_pennyworth_pm.pdf'\n",
      "An error occurred: 'str' object has no attribute 'raw_input_text'\n",
      "Error reading file '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesjohn_doe.pdf': [Errno 2] No such file or directory: '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesjohn_doe.pdf'\n",
      "An error occurred: 'str' object has no attribute 'raw_input_text'\n",
      "Error reading file '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesbarry_allen_fe.pdf': [Errno 2] No such file or directory: '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesbarry_allen_fe.pdf'\n",
      "An error occurred: 'str' object has no attribute 'raw_input_text'\n",
      "Error reading file '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesbruce_wayne_fullstack.pdf': [Errno 2] No such file or directory: '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesbruce_wayne_fullstack.pdf'\n",
      "An error occurred: 'str' object has no attribute 'raw_input_text'\n",
      "Error reading file '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesharvey_dent_mle.pdf': [Errno 2] No such file or directory: '/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Processed/Resumesharvey_dent_mle.pdf'\n",
      "An error occurred: 'str' object has no attribute 'raw_input_text'\n",
      "Parsing of the resumes is now complete\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "print(PROCESSED_RESUMES_PATH)\n",
    "# remove_old_files(PROCESSED_RESUMES_PATH)\n",
    "file_names = get_filenames_from_dir(\n",
    "    \"/home/kareem/hacking/research/nlp_projects/repotech/Resume_Matcher/Data/Resumes\"\n",
    ")\n",
    "# print(file_names)\n",
    "# except:\n",
    "# print(\"Error removing old files\")\n",
    "\n",
    "for file in file_names:\n",
    "    processor = ResumeProcessor(file)\n",
    "    # print(file)\n",
    "    success = processor.process()\n",
    "print(\"Parsing of the resumes is now complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
